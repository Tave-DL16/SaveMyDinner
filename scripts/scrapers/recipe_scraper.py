"""
ë ˆì‹œí”¼ í¬ë¡¤ëŸ¬
- 1000ê°œ ë ˆì‹œí”¼ ë°ì´í„° ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from typing import List, Dict, Optional
from pathlib import Path
import random

class RecipeScraper:
    """ë§Œê°œì˜ë ˆì‹œí”¼ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.base_url = "https://www.10000recipe.com" # ë§Œê°œì˜ ë ˆì‹œí”¼ë¡œ
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        self.recipes = []
        self.progress_file = Path("data/progress.json")
        self.output_file = Path("data/raw_recipes.json")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.progress_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ì§„í–‰ìƒí™© ë¡œë“œ
        self.load_progress()
    
    def load_progress(self):
        """ì§„í–‰ìƒí™© ë¡œë“œ"""
        if self.progress_file.exists():
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.recipes = data.get('recipes', [])
                print(f"âœ… Loaded {len(self.recipes)} existing recipes")
        else:
            print("ğŸ†• Starting fresh scraping")
    
    def save_progress(self):
        """ì§„í–‰ìƒí™© ì €ì¥"""
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump({
                'recipes': self.recipes,
                'total': len(self.recipes)
            }, f, ensure_ascii=False, indent=2)
        
        # ìµœì¢… ê²°ê³¼ë„ ì €ì¥
        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(self.recipes, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Progress saved: {len(self.recipes)} recipes")
    
    def get_recipe_list(self, category: str = None, page: int = 1) -> List[str]:
        """
        ë ˆì‹œí”¼ ëª©ë¡ í˜ì´ì§€ì—ì„œ URL ìˆ˜ì§‘
        
        Args:
            category: ì¹´í…Œê³ ë¦¬ (Noneì´ë©´ ì „ì²´)
            page: í˜ì´ì§€ ë²ˆí˜¸
            
        Returns:
            ë ˆì‹œí”¼ URL ë¦¬ìŠ¤íŠ¸
        """
        try:
            if category:
                url = f"{self.base_url}/recipe/list.html?cat={category}&page={page}"
            else:
                url = f"{self.base_url}/recipe/list.html?page={page}"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë ˆì‹œí”¼ ë§í¬ ì¶”ì¶œ
            recipe_links = []
            recipe_items = soup.select('.common_sp_list_ul li')
            
            for item in recipe_items:
                link = item.select_one('a')
                if link and link.get('href'):
                    recipe_url = link['href']
                    if recipe_url.startswith('/recipe/'):
                        recipe_links.append(f"{self.base_url}{recipe_url}")
            
            print(f"ğŸ“„ Found {len(recipe_links)} recipes on page {page}")
            return recipe_links
            
        except Exception as e:
            print(f"âŒ Error getting recipe list: {e}")
            return []
    
    def parse_ingredient(self, ingredient_text: str) -> str:
        """
        ì¬ë£Œ í…ìŠ¤íŠ¸ íŒŒì‹± (ê¸°ë³¸ ì •ë¦¬)
        
        Args:
            ingredient_text: ì›ë³¸ ì¬ë£Œ í…ìŠ¤íŠ¸
            
        Returns:
            ì •ë¦¬ëœ ì¬ë£Œ í…ìŠ¤íŠ¸
        """
        # ê³µë°± ì •ë¦¬
        ingredient_text = re.sub(r'\s+', ' ', ingredient_text).strip()
        
        # ê´„í˜¸ ì•ˆ ì„ íƒì‚¬í•­ ì œê±° (ì˜ˆ: "ì†Œê¸ˆ(ì•½ê°„)")
        # ingredient_text = re.sub(r'\([^)]*\)', '', ingredient_text)
        
        return ingredient_text
    
    def scrape_recipe_detail(self, url: str) -> Optional[Dict]:
        """
        ê°œë³„ ë ˆì‹œí”¼ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        
        Args:
            url: ë ˆì‹œí”¼ URL
            
        Returns:
            ë ˆì‹œí”¼ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë ˆì‹œí”¼ ID ì¶”ì¶œ
            recipe_id = url.split('/')[-1]
            
            # ë ˆì‹œí”¼ ì´ë¦„
            name_elem = soup.select_one('.view2_summary h3')
            name = name_elem.text.strip() if name_elem else "Unknown"
            
            # ì¬ë£Œ ì¶”ì¶œ
            ingredients = []
            ingredient_elems = soup.select('.ready_ingre3 ul li')
            for elem in ingredient_elems:
                # ì¬ë£Œëª…ê³¼ ì–‘ ì¶”ì¶œ
                ing_text = elem.get_text(strip=True)
                if ing_text:
                    ingredients.append(self.parse_ingredient(ing_text))
            
            # ì¡°ë¦¬ ìˆœì„œ
            steps = []
            step_elems = soup.select('.view_step_cont')
            for i, elem in enumerate(step_elems, 1):
                step_text = elem.get_text(strip=True)
                if step_text:
                    steps.append(f"{i}. {step_text}")
            
            # ì¹´í…Œê³ ë¦¬
            category_elem = soup.select_one('.view2_summary_info .category')
            category = category_elem.text.strip() if category_elem else "ê¸°íƒ€"
            
            # ë‚œì´ë„, ì‹œê°„, ì¸ë¶„
            info_elems = soup.select('.view2_summary_info span')
            difficulty = "ë³´í†µ"
            cooking_time = "30ë¶„"
            servings = 2
            
            for elem in info_elems:
                text = elem.get_text(strip=True)
                if "ë‚œì´ë„" in text or "ì´ˆê¸‰" in text or "ì¤‘ê¸‰" in text or "ê³ ê¸‰" in text:
                    if "ì´ˆê¸‰" in text or "ì‰½" in text:
                        difficulty = "ì‰¬ì›€"
                    elif "ê³ ê¸‰" in text or "ì–´ë ¤" in text:
                        difficulty = "ì–´ë ¤ì›€"
                    else:
                        difficulty = "ë³´í†µ"
                elif "ë¶„" in text:
                    cooking_time = text
                elif "ì¸ë¶„" in text:
                    servings_match = re.search(r'\d+', text)
                    if servings_match:
                        servings = int(servings_match.group())
            
            # ì¹¼ë¡œë¦¬ & ì˜ì–‘ ì •ë³´ (ìˆìœ¼ë©´)
            calories = None
            nutrition = {}
            
            nutrition_elem = soup.select_one('.view2_summary_info2')
            if nutrition_elem:
                nutrition_text = nutrition_elem.get_text()
                
                # ì¹¼ë¡œë¦¬ ì¶”ì¶œ
                cal_match = re.search(r'(\d+)\s*kcal', nutrition_text)
                if cal_match:
                    calories = int(cal_match.group(1))
                
                # ì˜ì–‘ì†Œ ì¶”ì¶œ (ë‚˜íŠ¸ë¥¨, ë‹¨ë°±ì§ˆ ë“±)
            
            # ì„¤ëª… 
            description_elem = soup.select_one('.view2_summary_in')
            description = description_elem.text.strip() if description_elem else ""
            
            recipe_data = {
                'id': recipe_id,
                'name': name,
                'ingredients': ingredients,
                'steps': steps,
                'category': category,
                'difficulty': difficulty,
                'cooking_time': cooking_time,
                'servings': servings,
                'calories': calories,
                'nutrition': nutrition,
                'description': description,
                'blog_url': url,
                'youtube_url': None,  # í•„ìš”í•˜ë‹¤ë©´...
                'source': '10000recipe'
            }
            
            print(f"âœ… Scraped: {name}")
            return recipe_data
            
        except Exception as e:
            print(f"âŒ Error scraping {url}: {e}")
            return None
    
    def scrape_multiple_pages(self, target_count: int = 1000, start_page: int = 1):
        """
        ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            target_count: ëª©í‘œ ë ˆì‹œí”¼ ìˆ˜
            start_page: ì‹œì‘ í˜ì´ì§€
        """
        current_count = len(self.recipes)
        page = start_page
        
        print(f"\nğŸ¯ Target: {target_count} recipes")
        print(f"ğŸ“Š Current: {current_count} recipes")
        print(f"ğŸ“„ Starting from page {page}\n")
        
        # ì´ë¯¸ ìˆ˜ì§‘í•œ URL ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
        collected_urls = {r['blog_url'] for r in self.recipes}
        
        while current_count < target_count:
            print(f"\n{'='*60}")
            print(f"ğŸ“„ Scraping page {page}...")
            print(f"{'='*60}")
            
            # ë ˆì‹œí”¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            recipe_urls = self.get_recipe_list(page=page)
            
            if not recipe_urls:
                print("âš ï¸  No more recipes found")
                break
            
            # ê° ë ˆì‹œí”¼ ìƒì„¸ í¬ë¡¤ë§
            for url in recipe_urls:
                if current_count >= target_count:
                    break
                
                # ì¤‘ë³µ ì²´í¬
                if url in collected_urls:
                    print(f"â­ï¸  Skipping duplicate: {url}")
                    continue
                
                # ë ˆì‹œí”¼ í¬ë¡¤ë§
                recipe_data = self.scrape_recipe_detail(url)
                
                if recipe_data:
                    self.recipes.append(recipe_data)
                    collected_urls.add(url)
                    current_count += 1
                    
                    print(f"ğŸ“Š Progress: {current_count}/{target_count}")
                    
                    # 10ê°œë§ˆë‹¤ ì €ì¥
                    if current_count % 10 == 0:
                        self.save_progress()
                
                # Rate limiting (ì˜ˆì˜)
                time.sleep(random.uniform(0.5, 1.5))
            
            page += 1
            
            # í˜ì´ì§€ ì‚¬ì´ì— ì•½ê°„ ê¸´ ëŒ€ê¸°
            time.sleep(random.uniform(2, 4))
        
        # ìµœì¢… ì €ì¥
        self.save_progress()
        
        print(f"\n{'='*60}")
        print(f"ğŸ‰ Scraping completed!")
        print(f"ğŸ“Š Total recipes: {len(self.recipes)}")
        print(f"ğŸ’¾ Saved to: {self.output_file}")
        print(f"{'='*60}")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    scraper = RecipeScraper()
    
    # 1000ê°œ ë ˆì‹œí”¼ í¬ë¡¤ë§
    scraper.scrape_multiple_pages(target_count=1000, start_page=1)
    
    print("\nâœ… Done!")
    print(f"ğŸ“ Output: data/raw_recipes.json")

if __name__ == "__main__":
    main()